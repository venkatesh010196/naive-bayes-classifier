{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPi0VbDI2hdNevB47xxLvp6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatesh010196/naive-bayes-classifier/blob/main/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCrXg-UB_XXl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "connect from API to huggigface to download the pretrained LLM model"
      ],
      "metadata": {
        "id": "cN1xx4mJoC2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xyoUPBY-vuz",
        "outputId": "9016a1e8-ad57-4b23-9fe0-b217cae03bd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "czO2UAWXoBH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading the llama LLM into memory to generate AI generated essays"
      ],
      "metadata": {
        "id": "TZLY0LaVoQ7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6h-BqIOZ-YDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting per process GPU memory fraction using PyTorch"
      ],
      "metadata": {
        "id": "ak9Oh5Lvoghk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.set_per_process_memory_fraction(1.00)"
      ],
      "metadata": {
        "id": "AtI8BrV5Gkh2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the LLM by providing a sample prompt"
      ],
      "metadata": {
        "id": "a0_hFOHXopmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "    'who is president of india?',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=50,\n",
        ")\n",
        "\n",
        "print(sequences)\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCMqDaci_mjl",
        "outputId": "f63b85cc-174a-4617-cf0c-65b6fabc2006"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'who is president of india?\\n\\nThe current President of India is Ram Nath Kovind. He was elected as the President of India in 2017 and took office on July 25, 2017'}]\n",
            "Result: who is president of india?\n",
            "\n",
            "The current President of India is Ram Nath Kovind. He was elected as the President of India in 2017 and took office on July 25, 2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self, train_data):\n",
        "        self.merged_df = self.dataAugmentation(train_data)\n",
        "        self.train()\n",
        "        self.wordProbList = []\n",
        "        self.finalAiReverseIndexedDict = {}\n",
        "        self.finalAiWordOccuranceList = []\n",
        "        self.finalHumanReverseIndexedList = {}\n",
        "        self.finalHumanWordOccuranceList = []\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        ps = PorterStemmer()\n",
        "        words = word_tokenize(text)\n",
        "        filtered_words = [ps.stem(w.lower()) for w in words if w.isalpha() and w.lower() not in stop_words]\n",
        "        return filtered_words\n",
        "\n",
        "    def preprocess_text(self, train_data_fold):\n",
        "        word_lists = []\n",
        "        for text in train_data_fold['text']:\n",
        "            words = self.tokenize(text)\n",
        "            word_lists.append(words)\n",
        "        return word_lists\n",
        "\n",
        "    #auxilury method to augement the data available by adding the essays generated by an LLM model.\n",
        "    def dataAugmentation(self, merged_df):\n",
        "        count = len(merged_df)\n",
        "        selected_prompts = self.merged_df.sample(100)[['prompt_name', 'instructions', 'source_text']]\n",
        "        generated_essays = []\n",
        "\n",
        "        for index, row in selected_prompts.iterrows():\n",
        "            prompt_name = row['prompt_name']\n",
        "            instructions = row['instructions']\n",
        "            source_text = row['source_text']\n",
        "\n",
        "            message = f\"Source: {source_text}\\nInstructions: {instructions}\"\n",
        "\n",
        "            sequences = pipeline(\n",
        "                message,\n",
        "                do_sample=True,\n",
        "                top_k = 5,\n",
        "                num_return_sequences=3,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                max_length=200,\n",
        "            )\n",
        "            # Extract the generated essay\n",
        "            generated_essay = sequences[0]['generated_text']\n",
        "\n",
        "            # Append to the list of generated essays\n",
        "            self.merged_df = self.merged_df.append({\n",
        "                'id': count + 1,\n",
        "                'prompt_id': f'gpt{count + 1}',\n",
        "                'text': generated_essay,\n",
        "                'generated': 1,\n",
        "                'prompt_name': prompt_name,\n",
        "                'instructions': instructions,\n",
        "                'source_text': source_text\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            # Timer to pause for 30 seconds\n",
        "            time.sleep(3)\n",
        "\n",
        "    def buildClassifier(self, train_data_fold):\n",
        "        self.ReverseIndexedList, self.allWordOccuranceList = self.makeReverseIndexedDict(train_data_fold, 2)\n",
        "        self.wordProbList = [self.ReverseIndexedList[word] / len(train_data_fold) for word in self.ReverseIndexedList.keys()]\n",
        "        self.aiReverseIndexedList, self.aiWordOccuranceList = self.makeReverseIndexedDict(train_data_fold, 1)\n",
        "        self.humanReverseIndexedList, self.humanWordOccuranceList = self.makeReverseIndexedDict(train_data_fold, 0)\n",
        "\n",
        "    def makeReverseIndexedDict(self, data_fold, generated):\n",
        "        filtered_word_lists = self.preprocess_text(data_fold)\n",
        "        frequency_dict = defaultdict(int)\n",
        "        # Iterate over each list of words\n",
        "        for word_list in filtered_word_lists:\n",
        "            # Create a set for the current list to avoid counting a word more than once\n",
        "            unique_words = set(word_list)\n",
        "            # Increment the count for each unique word in the current list\n",
        "            for word in unique_words:\n",
        "                frequency_dict[word] += 1\n",
        "        frequency_dict = dict(frequency_dict)\n",
        "\n",
        "        filtered_words = []\n",
        "\n",
        "        if generated == 2 or generated == 0:\n",
        "            for word, count in frequency_dict.items():\n",
        "                # Add the word to the list if count is greater than or equal to 5\n",
        "                if count >= 5:\n",
        "                    filtered_words.append(word)\n",
        "        else:\n",
        "            filtered_words = list(frequency_dict.keys())\n",
        "\n",
        "        reverse_indexed_dict = {word: index for index, word in enumerate(filtered_words)}\n",
        "        word_occurrence_list = [frequency_dict[word] for word in reverse_indexed_dict]\n",
        "\n",
        "        return reverse_indexed_dict, word_occurrence_list\n",
        "\n",
        "    def train(self):\n",
        "        accuracy = 0.0\n",
        "        # Assume X is your feature matrix, and y is your target variable\n",
        "        X = self.merged_df['text']\n",
        "        y = self.merged_df['generated']\n",
        "        # Split your data into train and test using StratifiedKFold\n",
        "        stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        for train_index, test_index in stratified_kfold.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            self.buildClassifier(X_train)\n",
        "\n",
        "            fold_accuracy = self.test(X_test, y_test, self.humanReverseIndexedList, self.humanWordOccuranceList,\n",
        "                                      self.aiReverseIndexedList, self.humanWordOccuranceList)\n",
        "            if fold_accuracy > accuracy:\n",
        "                accuracy = fold_accuracy\n",
        "                self.finalAiReverseIndexedDict = self.aiReverseIndexedList\n",
        "                self.finalAiWordOccuranceList = self.aiWordOccuranceList\n",
        "                self.finalHumanReverseIndexedList = self.humanReverseIndexedList\n",
        "                self.finalHumanWordOccuranceList = self.humanWordOccuranceList\n",
        "\n",
        "    def calculateClassProbability(self, word, ReverseIndexedList, WordOccuranceList, dataframe):\n",
        "        if word in ReverseIndexedList:\n",
        "            # Laplace smoothing: add 1 to the word's occurrence count and divide by the total count of unique words\n",
        "            probability = (1 + WordOccuranceList[ReverseIndexedList[word]]) / (2 + len(dataframe))\n",
        "        else:\n",
        "            probability = 1 / (2 + len(dataframe))\n",
        "        return probability\n",
        "\n",
        "    def predict(self, wordList, humanReverseIndexedList, humanWordOccuranceList, aiReverseIndexedList,\n",
        "                aiWordOccuranceList, dataframe):\n",
        "        ai_class_prob = len(self.merged_df[self.merged_df['generated'] == 1]) / len(self.merged_df)\n",
        "        human_class_prob = len(self.merged_df[self.merged_df['generated'] == 0]) / len(self.merged_df)\n",
        "\n",
        "        # Calculate the likelihood for each class based on the word occurrences\n",
        "        for word in set(wordList):\n",
        "            human_class_prob *= self.calculateClassProbability(word, humanReverseIndexedList, humanWordOccuranceList,\n",
        "                                                               self.merged_df[self.merged_df['generated'] == 0])\n",
        "            ai_class_prob *= self.calculateClassProbability(word, aiReverseIndexedList, aiWordOccuranceList,\n",
        "                                                            self.merged_df[self.merged_df['generated'] == 1])\n",
        "\n",
        "        predicted_class = 0 if human_class_prob < ai_class_prob else 1\n",
        "\n",
        "        return predicted_class\n",
        "\n",
        "    def test(self, X_test, y_test, humanReverseIndexedList, humanWordOccuranceList, AiReverseIndexedList,\n",
        "             AiWordOccuranceList):\n",
        "        count = 0\n",
        "        for sample in X_test:\n",
        "            sample_word_list = self.tokenize(sample)\n",
        "            if self.predict(sample_word_list, humanReverseIndexedList, humanWordOccuranceList, AiReverseIndexedList,\n",
        "                            humanWordOccuranceList) == y_test:\n",
        "                count = count + 1\n",
        "        return count / len(X_test)\n"
      ],
      "metadata": {
        "id": "8dJCbfy4Clw-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essays_df = pd.read_csv('/Users/venkatesh_vinnakota/Downloads/llm-detect-ai-generated-text/train_essays.csv')\n",
        "prompts_df = pd.read_csv('/Users/venkatesh_vinnakota/Downloads/llm-detect-ai-generated-text/train_prompts.csv')\n",
        "merged_df = pd.merge(essays_df, prompts_df, on='prompt_id')\n",
        "classifier = NaiveBayes(merged_df)\n",
        "accuracy = classifier.test()\n",
        "print(f'accuracy=======>{accuracy}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DUwDxeI6vGK",
        "outputId": "4369392f-a5d5-459b-a699-26b63792a436"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy=======>76.22%\n"
          ]
        }
      ]
    }
  ]
}